Metadata-Version: 2.4
Name: zenrube-mcp
Version: 0.1.0
Summary: Zen-Inspired Consensus for Rube Workflows - Multi-model AI orchestration
Author-email: vmanoilov <vmanoilov@users.noreply.github.com>
License: Apache-2.0
Project-URL: Homepage, https://github.com/vmanoilov/zenrube-mcp
Project-URL: Repository, https://github.com/vmanoilov/zenrube-mcp
Project-URL: Issues, https://github.com/vmanoilov/zenrube-mcp/issues
Keywords: ai,consensus,mcp,multi-model,rube,automation
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic>=1.10
Requires-Dist: PyYAML>=6.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: flake8>=6.0; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Provides-Extra: providers
Requires-Dist: openai<1.0.0; extra == "providers"
Provides-Extra: redis
Requires-Dist: redis>=5.0; extra == "redis"
Dynamic: license-file

# zenrube-mcp

Zenrube is a Zen-inspired consensus engine that orchestrates multiple LLM experts in parallel, synthesising their perspectives into a single actionable recommendation. It is designed for the Rube automation platform but is provider-agnostic and can be embedded into any Python project.

## âœ¨ Features

- ğŸ”€ Parallel or sequential expert orchestration with execution tracing
- ğŸ§  Three synthesis styles: `balanced`, `critical`, and `collaborative`
- ğŸªª Configurable expert personas with custom registrations
- ğŸ—‚ï¸ YAML configuration discovery (`.zenrube.yml` in project or home directory)
- ğŸ§± Structured Pydantic models for responses and configuration
- ğŸ’¾ TTL-aware caching (in-memory, file-system, or Redis)
- ğŸ”Œ Pluggable LLM providers (Rube, OpenAI, or custom implementations)
- ğŸ§ª Comprehensive unit test suite and CI pipeline

## ğŸš€ Quick Start

### Installation

```bash
pip install zenrube-mcp
```

or for local development:

```bash
git clone https://github.com/vmanoilov/zenrube-mcp.git
cd zenrube-mcp
pip install -e .[dev]
```

### CLI Usage

```bash
zenrube "Should we adopt an event-driven architecture?" \
  --style balanced \
  --experts pragmatic_engineer systems_architect security_analyst \
  --provider rube \
  --model gpt-4o-mini
```

Flags:

- `--style`: synthesis tone (`balanced`, `critical`, `collaborative`)
- `--experts`: one or more registered expert slugs
- `--sequential`: force sequential execution
- `--provider`: provider registry key to use
- `--model`: model identifier passed to the provider
- `--debug`: enable verbose logging
- `--no-cache`: bypass the caching layer for a single run

### Python API

```python
from zenrube import zen_consensus

result = zen_consensus(
    "Should we use microservices?",
    experts=["pragmatic_engineer", "systems_architect"],
    synthesis_style="critical",
    provider="rube",
    model="claude-3-sonnet",
)

print(result["consensus"])
```

The returned dictionary conforms to `models.ConsensusResult` and includes an `execution_id`, expert responses, synthesis text, and metadata about degraded states.

## âš™ï¸ Configuration

Zenrube loads configuration from `.zenrube.yml` in the project root and the user's home directory. A minimal example:

```yaml
experts:
  - pragmatic_engineer
  - systems_architect
  - security_analyst
synthesis_style: balanced
parallel_execution: true
provider: rube
logging:
  level: INFO
  debug: false
cache:
  backend: memory
  ttl: 300
custom_experts:
  product_manager:
    name: Product Manager
    system_prompt: |
      You are a product manager balancing user value, delivery speed, and stakeholder impact.
```

Custom experts registered through config become available to the CLI and Python API immediately.

## ğŸ”Œ Providers

Providers implement the `providers.LLMProvider` interface and are registered with the `ProviderRegistry`. Built-in providers include:

- `RubeProvider`: delegates to Rube's `invoke_llm`
- `OpenAIProvider`: thin wrapper around the OpenAI Chat Completions API

Register your own provider:

```python
from typing import Optional

from providers import LLMProvider, ProviderRegistry

class MockProvider(LLMProvider):
    name = "mock"

    def query(self, prompt: str, *, model: Optional[str] = None, **kwargs):
        return "Mock response", {"model": model}

ProviderRegistry.register(MockProvider())
ProviderRegistry.set_default("mock")
```

To plug in Rube's native helper, call:

```python
from zenrube import configure_rube_client
from rube import invoke_llm as rube_invoke

configure_rube_client(rube_invoke)
```

## ğŸ’¾ Caching

The caching layer defaults to in-memory storage. Configure file or Redis backends in `.zenrube.yml`:

```yaml
cache:
  backend: file
  directory: .zenrube-cache
  ttl: 600
```

Set `backend: redis` and a `url` to use Redis. You can also disable caching per request with the CLI `--no-cache` flag or the API `use_cache=False` argument.

## ğŸ§ª Testing

```bash
pip install -e .[dev]
pytest --cov=src
```

Continuous integration runs formatting (`black`), linting (`flake8`), typing (`mypy`), and coverage on Python 3.8â€“3.12.

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

Apache License 2.0

## ğŸ™ Acknowledgements

- Concept by [@vmanoilov](https://github.com/vmanoilov)
- Inspired by [Zen MCP](https://github.com/BeehiveInnovations/zen-mcp-server)
